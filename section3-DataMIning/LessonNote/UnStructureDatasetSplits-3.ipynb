{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c5debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b783d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global random seed\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5012d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data dirs,get class name\n",
    "\n",
    "# get root path\n",
    "data_root = './birdsDatasets/'\n",
    "\n",
    "# Based on the current root directory, traverse all objects in the directory and filter out all directories\n",
    "# 根据当前根目录，遍历目录下的所有对象，过滤出所有的目录\n",
    "first_path = [path for path in [os.path.join(data_root,objs) for objs in os.listdir(data_root)] if os.path.isdir(path)]\n",
    "\n",
    "# Continue to traverse all objects under the directory based on the first level subdirectory and filter out all second level subdirectories\n",
    "# 继续根据一级子目录，遍历目录下所有对象，过滤出所有二级子目录\n",
    "second_path = []\n",
    "for objs in first_path:\n",
    "    \n",
    "    # get second path\n",
    "    second_path.append([path for path in [os.path.join(objs,class_path) for class_path in os.listdir(objs)] if os.path.isdir(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f02b204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, the second level subdirectory name is the category of all the samples provided by the dataset. Now find a way to obtain these category names\n",
    "# 此时的二级子目录名称就是数据集提供的所有样本的类别，现在想办法获取这些类别名称\n",
    "\n",
    "# prepare dict to save class name by dataset names\n",
    "sample_class_dicts = {}\n",
    "\n",
    "# iter path array,index iter index,sets iter items\n",
    "for index,sets in enumerate(second_path):\n",
    "    \n",
    "    # all dataset classes name append to array\n",
    "    classes_arr = []\n",
    "    \n",
    "    for classes in sets:\n",
    "        \n",
    "        # append\n",
    "        classes_arr.append(classes.split('/')[-1])\n",
    "    \n",
    "    # get dict,key is dataset name,value is array\n",
    "    sample_class_dicts[second_path[index][0].split('/')[-2]] = classes_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83391f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断训练集、验证集、测试集中给定的所有样本类别是否全部一致\n",
    "set(sample_class_dicts['valid']) == set(sample_class_dicts['train']) == set(sample_class_dicts['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00ef63ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 既然经过判断，各个子集下样本的类别不一致，则将所有类别全部拼接在一起，并去重，即得数据集给定的所有样本类别\n",
    "all_classes = list(set(sample_class_dicts['valid'] + sample_class_dicts['train'] + sample_class_dicts['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5853a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始构造类别映射对象：类别 -- 对象 and 对象 -- 类别\n",
    "cls_mappers = {\n",
    "    \n",
    "    # class to id\n",
    "    'cls2id':{},\n",
    "    \n",
    "    # id to class\n",
    "    'id2cls':{}\n",
    "}\n",
    "\n",
    "# iter index and value\n",
    "for index,value in enumerate(all_classes):\n",
    "    cls_mappers['cls2id'][value] = index\n",
    "    cls_mappers['id2cls'][index] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "558af016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save json\n",
    "json.dump(cls_mappers,open('./cls_mapper.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f6b571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备函数：递归遍历指定根目录，遍历其中所有的文件，构建每一个文件的名称\n",
    "def recursive_fetching(root, suffix=['jpg', 'png']):\n",
    "    all_file_path = []\n",
    "\n",
    "    def get_all_files(path):\n",
    "        all_file_list = os.listdir(path)\n",
    "        # 遍历该文件夹下的所有目录或者文件\n",
    "        for file in all_file_list:\n",
    "            filepath = os.path.join(path, file)\n",
    "            # 如果是文件夹，递归调用函数\n",
    "            if os.path.isdir(filepath):\n",
    "                get_all_files(filepath)\n",
    "            # 如果不是文件夹，保存文件路径及文件名\n",
    "            elif os.path.isfile(filepath):\n",
    "                all_file_path.append(filepath)\n",
    "\n",
    "    get_all_files(root)\n",
    "\n",
    "    file_paths = [it for it in all_file_path if os.path.split(it)[-1].split('.')[-1].lower() in suffix]\n",
    "\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a6df1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89885"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_items = recursive_fetching(data_root)\n",
    "len(dataset_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "095623f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有样本路径后，将样本打乱，随机重排，增加随机性\n",
    "random.shuffle(dataset_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7b2c841",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# # 数据集的每一个类别及对应的数据list\n",
    "dataset_dict = {\n",
    "    0: ['./birdsDatasets/valid/CREAM COLORED WOODPECKER/4.jpg','./birdsDatasets/valid/CREAM COLORED WOODPECKER/5.jpg'...]\n",
    "    1: ['./birdsDatasets/valid/CREAM COLORED WOODPECKER/4.jpg','./birdsDatasets/valid/CREAM COLORED WOODPECKER/5.jpg'...]\n",
    "    ...\n",
    "    xx:[xx,xx]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "dataset_dict = {}\n",
    "for it in dataset_items:\n",
    "    \n",
    "    # get sample classes name\n",
    "    cls_name = os.path.split(it)[0].split('/')[-1]\n",
    "    \n",
    "    # cls to id\n",
    "    cls_id = cls_mappers['cls2id'][cls_name]\n",
    "    \n",
    "    # if cls_id not in data\n",
    "    if cls_id not in dataset_dict:\n",
    "        \n",
    "        # init\n",
    "        dataset_dict[cls_id] = [it]\n",
    "    else:\n",
    "        \n",
    "        # append\n",
    "        dataset_dict[cls_id].append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c0695be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个类别按照比例分到train/eval/test\n",
    "train_ratio, eval_ratio, test_ratio = 0.8, 0.1, 0.1\n",
    "\n",
    "# prepare array\n",
    "train_set, eval_set, test_set = [], [], []\n",
    "\n",
    "# iter\n",
    "for _, set_list in dataset_dict.items():\n",
    "    \n",
    "    # get data length:how many sample in dataset\n",
    "    length = len(set_list)\n",
    "    \n",
    "    # calculate every set of sample numbers\n",
    "    train_num, eval_num = int(length*train_ratio), int(length*eval_ratio)\n",
    "    test_num = length - train_num - eval_num\n",
    "    \n",
    "    # shuffle\n",
    "    random.shuffle(set_list)\n",
    "    \n",
    "    # generate finall set\n",
    "    train_set.extend(set_list[:train_num])\n",
    "    eval_set.extend(set_list[train_num:train_num+eval_num])\n",
    "    test_set.extend(set_list[train_num+eval_num:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86651288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set samples number:71719;\n",
      "test set samples number:9388;\n",
      "eval set samples number:8778;\n"
     ]
    }
   ],
   "source": [
    "# 再次随机打乱\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(eval_set)\n",
    "random.shuffle(test_set)\n",
    "\n",
    "print(f'train set samples number:{len(train_set)};\\ntest set samples number:{len(test_set)};\\neval set samples number:{len(eval_set)};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8302de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "def save_meata_data(meta_path,datasets):\n",
    "    \n",
    "    # file write\n",
    "    with open(meta_path,'w') as f:\n",
    "        for path in datasets:\n",
    "            \n",
    "            # get class name\n",
    "            cls_name = os.path.split(path)[0].split('/')[-1]\n",
    "            \n",
    "            # class name to id\n",
    "            cls_id = cls_mappers['cls2id'][cls_name]\n",
    "            \n",
    "            # write\n",
    "            f.write(f'{cls_id}|{path}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7093b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_path get path ,datasets get data\n",
    "for meta_path,datasets in zip(['./train_meta_data.txt','./test_meta_data.txt','./eval_meta_data.txt'],[train_set,test_set,eval_set]):\n",
    "    save_meata_data(meta_path,datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLEnvs",
   "language": "python",
   "name": "mlenvs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
